version: "3.8"
services:
  wikipedia_pageviews:
    build: .
    environment:
      SPARK_MODE: local
      SERVICE_NAME: wikipedia_pageviews
      APP_NAME: wikipedia_pageviews
      SOURCE_URL_PREFIX: "https://dumps.wikimedia.org/other/pageviews"
      BLACKLIST_URL_IN: https://s3.amazonaws.com/dd-interview-data/data_engineer/wikipedia/blacklist_domains_and_pages
      PATH_OUT: /mnt/out
      EXECUTION_DATETIME: "2020-01-21T11:00:00"
      HOURLY: "False"
      PYTHON_FILE: /workspace/jobs/wiki_pageviews_to_s3.py
      SPARK_PROPERTIES: |-
        spark.master=local[8]
        spark.driver.memory=14g
        spark.driver.maxResultSize=2g
        spark.sql.autoBroadcastJoinThreshold=-1
        spark.hadoop.fs.s3a.multiobjectdelete.enable=false
        spark.hadoop.fs.s3a.fast.upload=true
        spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
        spark.speculation=false
        spark.serializer=org.apache.spark.serializer.KryoSerializer
    volumes:
        - /tmp:/mnt

  wikipedia_pageviews_tests:
    build:
      context: .
      dockerfile: Dockerfile.test
    volumes:
        - .:/mnt
    entrypoint:  ["pytest", "/mnt/jobs/tests/"]
