version: '2'
#env: source from parent taskfile
#vars: source from parent taskfile
env:
  REMOTE_DAG_FOLDER: /usr/local/airflow

tasks:

  default:
    cmds:
      - echo executed on $DOCKER_REGISTRY
    silent: true
    vars:
      DAG_FILE:
        sh: echo $(pwd)/dags/spark_batch_job_distributed_mode.py

  backfill:
    summary: |
      When run locally, you need to specify environment variables
      eg. KUBE_CONTEXT=ew1p3 NAMESPACE=data-flux ENV=prod DAG_NAME=spark_batch_job_distributed_mode task dags:backfill

    cmds:
      - kubectl exec deploy/airflow -- airflow backfill "{{.DAG_NAME}}" --start_date "{{.START_DATE}}" --end_date "{{.END_DATE}}"    --rerun_failed_tasks
    vars:
      START_DATE:
        '{{default "2020-07-26" .START_DATE}}'
      END_DATE:
        '{{default "2020-07-30" .END_DATE}}'
      DAG_NAME:
        '{{default "spark_batch_job_distributed_mode" .DAG_NAME}}'

  package:
    summary: |
      When run locally, you need to specify environment variables
      eg. DAG_NAME=spark_batch_job_distributed_mode task dags:package
    dir: ./dags
    cmds:
      - mkdir -p dist
      - zip -r dist/{{.DAG_NAME}}.zip {{.FILES}} -MM {{.DAG_NAME}}.py
    vars:
      DAG_NAME: '{{default "__TO_BE_SET__" .DAG_NAME}}'
      FILES:
        ./shared
        ./plain_files

  single_deploy:
    summary: |
      When run locally, you need to specify environment variables
      eg. DAG_NAME=spark_batch_job_distributed_mode NAMESPACE=data-flux KUBE_CONTEXT=prod task dags:single_deploy
    dir: ./dags
    cmds:
      - task: package
      - task: :set.k8s.context
      - kubectl -n $NAMESPACE cp $(pwd)/dist/{{.DAG_NAME}}.zip $AIRFLOW_POD_NAME:$REMOTE_DAG_FOLDER/dags/{{.DAG_NAME}}.zip
    vars:
      DAG_NAME: |-
        '{{default "__TO_BE_SET__" .DAG_NAME}}'
    env:
      AIRFLOW_POD_NAME:
        sh: kubectl -n $NAMESPACE get pods -o jsonpath="{.items[0].metadata.name}" -l app=airflow || echo "no connection"

  single_undeploy:
    summary: |
      When run locally, you need to specify environment variables
      eg. DAG_NAME=spark_batch_job_distributed_mode NAMESPACE=data-flux KUBE_CONTEXT=prod task dags:single_deploy
    cmds:
      - task: :set.k8s.context
      - kubectl exec deploy/airflow --  rm -rfv $REMOTE_DAG_FOLDER/dags/{{.DAG_NAME}}.zip
    vars:
      DAG_NAME: |-
       '{{default "__TO_BE_SET__" .DAG_NAME}}'

  deploy:
    summary: |
      Deploy all dags to airflow, it does not include dags run
      It include lists of dags to deploy
      NAMESPACE=data-flux-stg task dags:_deploy
    cmds:
      - >
        {{range $d := .DAG_NAMES | trim | splitLines -}}
            DAG_NAME={{$d}} task dags:single_deploy
        {{end}}
    vars:
      DAG_NAMES: |-
        spark_batch_job_distributed_mode
        spark_batch_job_local_mode

  undeploy:
    summary: |
      remove all zipped files from airflow remote folder
      NAMESPACE=data-flux-stg task dags:undeploy
    cmds:
      - kubectl exec deploy/airflow --  rm -rfv $REMOTE_DAG_FOLDER/dags/*.zip
